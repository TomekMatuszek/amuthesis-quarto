# Materials and methods {#sec-data-methods}

Workflow of the study consisted of several stages: preprocessing of source data (described in Sections [-@sec-sat] and [-@sec-landcover]), creating training dataset, model parameters tuning and quality assessment ([@sec-resampling]), land cover map prediction and evaluating the impact of the thermal band on the model results ([@sec-importance]).
Visual representation of the workflow is shown in [@fig-rycina1].

Each of these steps was performed using R programming language [@R-base] and final visualizations were created in QGIS software [@qgis_development_team_qgis_2009].
Both programming environment and GIS software used in this process are open-source.

```{mermaid}
%%| label: fig-rycina1
%%| echo: false
%%| fig-cap: "General workflow of the study"
%%| fig-width: 4
flowchart TD
  A[/Downloading GLAD data/] ---> B[merging and reprojecting]
  D[/Downloading LUCAS data/] --> E[reclassification]
  subgraph source data processing&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
  B --> C[calculating spectral indices]
  E --> F[merging LUCAS Grid and Primary Data]
  F --> G[filtering based on auxillary info]
  C --> H[extracting spectral values of LUCAS points]
  G --> H
  H --> I[filtering based on NDWI and QF]
  end
  subgraph machine learning&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
  I --> J[creating ML dataset/task]
  J --> L[nested spatial cross-validation]
  L --> K[applying fine-tuned hyperparameters]
  K --> N[model training]
  N --> O[prediction]
  end
  L -----> M[\model quality assessment\]
  O --> P[\land cover map\]
  subgraph analysing variable importance&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
  N ----> R[calculating variable importance]
  R --> S[quantifying thermal band's importance<br>for each training point]
  S --> T[spatial interpolation]
  end
  T --> U[\map of thermal band's importance\]
  S ----> W[\B10 importance for each land cover class\]
```

Landsat ARD dataset, provided by GLAD laboratory at the Univeristy of Maryland, was used as a source of multi-spectral satellite imagery [@potapov_landsat_2020].
Training points were obtained from LUCAS dataset created by Eurostat [@dandrimont_harmonised_2020].
Both datasets were downloaded for central-western part of Poland which was chosen as the training area ([@fig-rycina2]).
This data was preprocessed and then used to train the model and validate its performance.

```{r}
#| label: fig-rycina2
#| echo: false
#| fig-cap: "Area covered by the downloaded satellite imagery"
#| out-width: "564px"
#| out-height: 400px
knitr::include_graphics("figures/study_area.png")
```

## Satellite imagery {#sec-sat}

Satellite imagery from GLAD Landsat ARD product is available in 16-day interval composites and is divided into 1° x 1° tiles.
Processing of original Landsat images performed by the GLAD team included converting spectral bands' information to top-of-atmosphere (TOA) reflectance, converting thermal band values to brightness temperature (BT) in Kelvins, scaling the values of all bands, as well as, adding quality flag (QF) for every pixel [@potapov_landsat_2020].

Satellite images for eight 1° x 1° tiles, covering the study area, were downloaded using GLAD Tools v1.1 and PERL programming language ([@fig-rycina1]).
These images are from 10th interval of the year 2018, so downloaded mosaics consist of images acquired between 24.05.2018 and 8.06.2018.
All downloaded images were merged and reprojected from the WGS 84 coordinate reference system (EPSG:4326) to UTM zone 33N (EPSG:32633).
Every band was also resampled from its original 0.00025° resolution (corresponding to 27.83 m on the equator) to 30 meters.

In addition, four spectral indices were derived: Normalized Difference Vegetation Index (NDVI), Modified Normalized Difference Water Index (MNDWI), Normalized Difference Moisture Index (NDMI) and Modified Bare soil Index (MBI).
Formulas used to calculate these indices can be found in [@tbl-tabela1].

```{r tabela1, echo=FALSE}
#| label: tbl-tabela1
#| echo: false
#| tbl-cap: "Formulas of spectral indices derived from Landsat data"
df = data.frame(
  a = c("Blue", "Green", "Red", "Near Infrared", "Short-wave Infrared 1", "Short-wave Infrared 2", "Thermal", "Normalized Difference Vegetation Index", "Modified Normalized Difference Water Index", "Normalized Difference Moisture Index", "Modified Bare Surface Index"),
  b = c("B2", "B3", "B4", "B5 (NIR)", "B6 (SWIR1)", "B7 (SWIR2)", "B10 (TIRS1)", "NDVI", "MNDWI", "NDMI", "MBI"),
  c = c(rep("-", 7), "(B5 -B4) / (B4 + B5)", "(B3 - B6) / (B3 + B6)", "(B5 - B6) / (B5 + B6)", "(B6 - B7 - B5) / (B6 + B7 + B5) + 0.5")
)
colnames(df) = c("band/index", "abbreviation", "formula")
df |>
  knitr::kable(format = "latex", booktabs = TRUE, linesep = "") |>
  kableExtra::row_spec(0, bold = TRUE, extra_css = "border-left: 1px solid grey; border-right: 1px solid grey") |>
  kableExtra::kable_classic(full_width = FALSE) |>
  kableExtra::row_spec(1:(nrow(df) - 1), extra_latex_after = "\\hline") |>
  kableExtra::column_spec(1, border_left = TRUE, width = "4cm") |>
  kableExtra::column_spec(2, border_left = TRUE) |>
  kableExtra::column_spec(3, border_left = TRUE, border_right = TRUE)
```

## Land cover data {#sec-landcover}

Data collected during the LUCAS field survey performed by Eurostat was chosen as a land cover training set.
At the moment of writing, it is the most accurate and comprehensive dataset containing information about land use and land cover [@pflugmacher_mapping_2019] due to the fact that every point was either manually photo-interpreted or assessed during an *in-situ* visit.

LUCAS field survey consists of two phases.
The first phase is based on a grid of points with 2 km spacing covering whole territory of the European Union (which equals to more than 1 million points).
Each point of the grid is visually interpreted using ortho-photos or satellite images, and classified into one of seven major land-cover classes.
These classes are: arable land, permanent crops, grassland, wooded areas/shrub land, bare land, artificial land and water [@oliver_buck_analysis_2015].
In the second phase, a subsample of grid points is selected and then visited by Eurostat surveyors.
They classify each point according to full LUCAS land cover and land use classification.
The survey takes place in the spring and summer in order to observe chosen places in their high vegetation season [@dandrimont_harmonised_2020].

Surveyors not only assign land cover and land use classes to points, but they also add auxillary information such as plant species present at the site, percentage of land coverage of a chosen class, height of the trees and their maturity, as well as information about local water management and irrigation].
If there are more than one land cover/land use types at the point, observer can also assign a secondary class for every LUCAS point [@oliver_buck_analysis_2015].

The majority of the training points used in the classification model were from the second phase of LUCAS survey, also called LUCAS Primary Data.
I downloaded a total of 4,153 points for the study area.
The pre-processing step included omitting records with missing data, excluding artificial linear land cover classes (e.g. roads or railways) and excluding points that were surveyed more than 500 meters from their theoretical location.
In the next step, detailed land cover classes were aggregated into eight main groups of land cover types.
Two of them - grassland and shrubland were additionally aggregated into one land cover class due to their spectral and descriptive similarity.
Then, I filtered some of the points according to the percentage of land cover class coverage or percentage of impervious surface coverage ([@tbl-tabela2]).
This step reduced number of unreliable training points with mixed land cover, e.g. points with assigned class covering less than 50% of surface around it.

```{r tabela2, echo=FALSE}
#| label: tbl-tabela2
#| echo: false
#| tbl-cap: "Filters applied to reclassified land cover groups. IMP - impervious surface, HRB - herbaceous plants cover, TC - tree cover"
df = data.frame(
  ID = c("1", "2", "3", "4", "5", "6", "7"),
  land_cover = c("arable land", "grassland", "forests", "bare land", "artificial land", "water bodies", "wetlands"),
  lucas_grid = c(rep("-", 3), "6 (Bare surface)", "7 (Artificial areas)", "8 (Inland water)", "-"),
  lucas_primary = kableExtra::linebreak(c("B00 (Cropland)", "E00 (Grassland), D00 (Shrubland)", "C00 (Woodland)", "F00 (Bare land)", "A00 (Artificial land)", "G00 (Water areas)", "H00 (Wetlands)"), double_escape = F, linebreaker = "\n"),
  filters = c("<30% IMP", ">50% HRB; <30% IMP", ">50% TC; <20% IMP", "-", ">70% IMP", "-", "-")
)
colnames(df) = c("ID", "LC class", "LUCAS Grid", "LUCAS Primary Data", "Filters")
cols = c("#e8ef5f", "#80dc59", "#11a723", "#b7b7b7", "#ea001f","#56a4f3", "#7a338c")
df |>
  knitr::kable(format = "latex", booktabs = TRUE, linesep = "") |> 
  kableExtra::row_spec(0, bold = TRUE, extra_css = "border-left: 1px solid grey; border-right: 1px solid grey") |>
  kableExtra::row_spec(1:(nrow(df) - 1), extra_latex_after = "\\hline") |>
  kableExtra::column_spec(2:3, border_left = TRUE) |>
  kableExtra::column_spec(4, width = "4cm", border_left = TRUE) |>
  kableExtra::column_spec(5, width = "2cm", border_left = TRUE, border_right = TRUE) |>
   kableExtra::column_spec(1, bold = TRUE, background = cols, border_left = TRUE) |>
  kableExtra::kable_classic(full_width = FALSE)
```

For the least frequent classes in the LUCAS Primary Data dataset - bare land, artificial land and water bodies - I also added points classified during the first phase of LUCAS survey ([@fig-rycina3]).
This step was necessary to ensure that every land cover class is represented by enough points.
It was not possible only for the wetlands class, because of the lack of such category in the first phase classification.
At the end of the pre-processing, dataset had 3,778 training points.

```{r}
#| label: fig-rycina3
#| echo: false
#| fig-cap: "Distribution of points by land cover class after pre-processing"
#| out-width: "533px"
#| out-height: 300px
knitr::include_graphics("figures/lucas_data.png")
```

After extracting values from Landsat ARD raster, LUCAS points were also filtered using the quality flag provided.
Only points with the clear-sky quality flag were taken into account during the model training.
Moreover, water bodies points in which NDWI was lower than 0 were also excluded.
These two conditions eliminated 404 points in total.

The training set obtained after pre-processing can be seen in [@fig-rycina4].
Spatial distribution of data points was fairly even and due to the structure of LUCAS data set, every point was located 2 kilometers or further from the next one.

```{r}
#| label: fig-rycina4
#| echo: false
#| fig-cap: "Spatial distribution of LUCAS training points after pre-processing"
#| out-width: "564px"
#| out-height: 400px
knitr::include_graphics("figures/lucas_distribution.png")
```

## Machine learning {#sec-ml}

Machine learning is a computation method used to teach machines from datasets automatically, without being specifically programmed [@mahesh_machine_2018; @sarker_machine_2021].
We can divide machine learning methods into two main groups: supervised and unsupervised.

Unsupervised learning analyzes unlabeled datasets without the need for human intervention.
This is widely used for extracting generative features, identifying meaningful trends and structures, grouping results and exploratory purposes [@sarker_machine_2021].
This type of machine learning discovers hidden patterns or data groupings (clusters) which is used in exploration analysis or objects segmentation.

Supervised learning uses labeled training data and a collection of training examples, which are used by an algorithm to find relationships between different variables.
It is carried out when certain goals are identified to be accomplished from a certain set of inputs.
There are two main types of supervised learning tasks: classification (separating data) and regression (fitting data) [@sarker_machine_2021].

In this study, supervised classification algorithm called Random Forest (RF) was used [@breiman_random_2001].

### Random forest algorithm {#sec-rf}

I chose Random Forest as an algorithm used in this study, since it is considered to be the best classification algorithm for land cover mapping [@talukdar_land-use_2020].
It is a very popular machine learning tool thanks to its high interpretability and relatively high accuracy [@qi_random_2012].
Other advantages of this algorithm is its ability to handle missing values, wide spectrum of accepted variable types (continuous, binary, categorical) and ease of modelling high-dimensional data [@qi_random_2012].
Random Forest consists of a specified number of decision trees, which are based on series of splitting rules.

A decision tree aims to partition the dataset into smaller, more homogeneous groups [@kuhn_applied_2013].
This process creates a set of rules by dividing dataset into several categories.
Each rule in the decision tree is specified by a feature (variable used to split) and a threshold (value of a feature dividing dataset) [@sekulic_random_2020].
Random Forest algorithm is characterized by using many decision trees at the same time and receiving results by applying majority voting system based on outputs of all decision trees [@kuhn_applied_2013].
Each tree in the forest has slightly different input data - a subset of data is sampled with replacement to get different result in every tree.
This process is known as bagging or bootstrap aggregating [@schonlau_random_2020].
Moreover, algorithm is allowed to use only a subset (randomly sampled) of available variables in every split which reduces correlation between trees [@sohil_introduction_2022].

### Model quality assessment and fine-tuning {#sec-resampling}

Accuracy of the model was assessed using five performance measures:

-   Overall accuracy: ratio of number of correct predictions to the total number of input points

-   Kappa coefficient: how well the classification performed as compared to assigning values randomly

-   Recall (producer's accuracy): how often are real features on the ground correctly shown on the classified map

-   Precision (user's accuracy): how often the class on the map will actually be present on the ground

-   F1-score: harmonic mean between precision and recall, measures if classifier both classifies data correctly and does not miss a significant number of points

Every above metric, except Kappa coefficient, takes values from 0 to 1.
Value of 0 means poor model performance and value of 1 means high quality of the model.
As for Kappa coefficient, values range from -1 to 1.
Values below 0 mean worse agreement between data distributions than random chance and values above 0 (up to 1) mean model performing better than random.

Values of these indices were estimated with the help of resampling technique called spatial cross-validation (CV) [@lovelace_geocomputation_2019].
It is a type of cross-validation that divides dataset into folds and also considers spatial aspect of the data.

In *k*-fold cross-validation, every data point is used in both training and testing set.
Whole dataset is randomly divided into *k* equal parts (*folds*).
Then, machine learning model is independently trained *k* times and in each run, different part of the dataset is used as validation set, while remaining *k - 1* parts are used to fit the model.
This way, every data point is used in the testing set only once and is used to train the model in the remaining runs [@jiao_performance_2016].
Usually, whole cross-validation procedure is repeated several times to get higher number of unique dataset splits and to receive more reliable average values of the overall accuracy [@varga_validation_2021].
Such approach is a compromise which enables possibility of using a whole dataset in the training process of the final model without a need of acquiring independent testing set in order to measure model's performance.

Since this study is based on geographic data, spatial autocorrelation needs to be taken into account.
As Tobler stated: "Everything is related to everything else, but near things are more related than distant things" [@tobler_computer_1970].
In order to prevent testing points from being related to training points, I applied spatial cross-validation approach which aims to prevent the model to overfit to the training data.
This method is different than regular cross-validation only in the partitioning step - instead of randomly dividing dataset into groups, location of data points is used together with k-means clustering [@brenning_spatial_2012] in order to create spatially disjoint folds [@lovelace_geocomputation_2019].
Thanks to this partitioning method, spatial bias can be significantly reduced which leads to more reliable performance estimation.
Example of such approach can be seen in [@fig-rycina5].

```{r}
#| label: fig-rycina5
#| echo: false
#| fig-cap: "Comparison of random and spatial partitioning of dataset for cross-validation on external example data [Source: @lovelace_geocomputation_2019]"
#| out-width: "570px"
#| out-height: 350px
knitr::include_graphics("figures/spatial_partitioning.png")
```

Random Forest algorithm takes several hyperparameters as an input in order to specify how much should it fit to training data.
Optimizing these parameters is crucial for tree-based machine learning models [@yang_hyperparameter_2020].
Model's hyperparameters can be fine-tuned to find values that give the best model accuracy.

With the aim to determine values of model's hyperparameters as accurately as possible, I performed nested spatial cross-validation.
This method is an extension of previously described approach, with hyperparameter tuning added to the process.
Each fold created in the spatial CV is further divided into next *n* folds which comprise the tuning level of the process.
Then, another *n*-fold cross-validation is performed on these folds in order to determine performance of randomly sampled hyperparameter values.
The best hyperparameter combination is chosen to train the model on outer fold (performance estimation level) [@schratz_hyperparameter_2019].
Whole process is then repeated on every of *k* outer folds which leads to the most accurate performance measurement as well as defining the best hyperparameter setting.

I chose three hyper-parameters for tuning: number of trees, maximum depth of the forest and minimal size of each node in decision tree.
I used overall accuracy achieved by each classifier to rank their performance and choose parameters that train the model best.
On the tuning level of every fold in spatial CV process, I examined 10 random configurations of hyperparameters and assessed their performance by applying 5-fold inner resampling.
Parameters' search spaces and tuning result received from nested cross-validation can be found in [@tbl-tabela3].

```{r tabela3, echo=FALSE}
#| label: tbl-tabela3
#| echo: false
#| tbl-cap: "Hyperparameters of RF model optimized during nested spatial cross-validation"
df = data.frame(
  parameter = c("number of trees", "maximum depth", "min. node size"),
  search_space = c("50 - 400", "10 - 40", "1 - 10"),
  choice = c("186", "99", "2")
)
colnames(df) = c("Hyper-parameter", "Search space", "Optimal value")
df |>
  knitr::kable(format = "latex", booktabs = TRUE, linesep = "") |> 
  kableExtra::row_spec(0, bold = TRUE, extra_css = "border-left: 1px solid grey; border-right: 1px solid grey") |>
  kableExtra::column_spec(1:2, border_left = TRUE) |>
  kableExtra::column_spec(3, border_left = TRUE, border_right = TRUE) |>
  kableExtra::kable_classic(full_width = FALSE) |>
  kableExtra::row_spec(1:(nrow(df) - 1), extra_latex_after = "\\hline")
```

## Variable importance and its spatial distribution {#sec-importance}

Quantifying importance of model's variables is a part of evaluating its results.
It can be used for model simplification and exploration, domain-knowledge-based validation or knowledge generation [@biecek_explanatory_2021].
This study was focused on the latter purpose since its aim was to check if thermal information has a significant impact on land cover classification.

Importance of model variables can be measured on two levels: dataset level and instance level [@biecek_explanatory_2021].
On the dataset level, we can measure change in model accuracy depending on the presence of one chosen variable ([@sec-importance-dataset]).
This gives basic knowledge about this variable's impact on model predictions.
Assessing importance on the instance (observation) level helps to understand an impact of variables for one specific data point ([@sec-importance-instance]).
Moreover, the instance level importance can be utilized to interpolate variable importance values from points into continuous raster data ([@sec-importance-distribution]).

### Dataset level {#sec-importance-dataset}

Measuring variable importance on the dataset level requires evaluating model twice: once with original data and once with permuted values of the considered variable.
The main idea behind this action is to measure difference between models' performance.
Breiman [-@breiman_random_2001] assumes that if a variable is important, then model's performance is expected to lower after permuting this variable's values.
For this purpose, cross entropy was used as a loss function thus its change was considered as a measure of variable importance [@biecek_explanatory_2021].
In order to measure each variable's importance, twelve seperate models were created: one with original data and eleven modified models, each one with different variable's values permuted.
Comparison of these eleven models and the original model made possible quantifying impact of every variable on the original model results.
This value is treated as an overall variable importance on the dataset level.

There is also a visual method to explore variable importance on dataset level.
It is based on interpreting partial-dependence (PD) profiles of variables ([@fig-rycina6]).
Such plot shows how does probability of choosing certain class changes as a function of the selected variable [@biecek_explanatory_2021].
Values for PD profile are calculated by averaging Ceteris-paribus profiles created for every observation in the dataset.
This approach is an easy and intuitive way to understand variables' impact on model results.
If probability values of choosing certain class do not change along with the changes of variable's value, we can assume that this variable does not have big impact on model predictions or that our model did not detect such dependence.

```{r}
#| label: fig-rycina6
#| echo: false
#| fig-cap: "Example partial-dependence profile for near-infrared band (B5)"
#| out-width: "540px"
#| out-height: 300px
knitr::include_graphics("figures/profB5.png")
```

### Instance level {#sec-importance-instance}

Another way to measure variable importance in machine learning models is the instance level evaluation.
It helps to find out how much each variable contributed to the model's outcome for a particular observation [@biecek_explanatory_2021].
One way of calculating variable impact on the observation result is creating a break-down plot ([@fig-rycina7]).
Its main idea is to estimate contribution of variable by measuring the change in model predictions while fixing the values of consecutive variables to values recorded for the chosen observation [@biecek_explanatory_2021].
After fixing the value of variable for whole dataset, change in model prediction is calculated.
This value indicates the variable impact on a chosen observation.

```{r}
#| label: fig-rycina7
#| echo: false
#| fig-cap: "Example of a break-down plot that visualises variables' impact on chosen observation"
#| out-width: "500px"
#| out-height: 535px
knitr::include_graphics("figures/break-down_plot.png")
```

However, above method is highly dependent on variable ordering and interactions between these variables [@biecek_explanatory_2021].
To address this issue, I applied another approach based on averaging values from multiple break-down plots, each one with different ordering of the variables.
This method originates from "Shapley values" [@shapley_value_1953] and was adapted to machine learning by Štrumbelj and Kononenko [-@strumbelj_efficient_2010].
Main idea of this approach is to apply several different variable orderings, create a break-down plot for each of them and calculate the mean value of contribution for each variable ([@fig-rycina8]).
Thanks to this method, the influence of variable ordering can be mostly removed [@biecek_explanatory_2021].

```{r}
#| label: fig-rycina8
#| echo: false
#| fig-cap: "Example plot of Shapley Additive Explanations"
#| out-width: "500px"
#| out-height: 535px
knitr::include_graphics("figures/shapley_values.png")
```

Eventually, Shapley values provide a possibilty to measure contribution of each variable in every observation in the training set.
Such result enables us to add spatial context to the variable importance, which is further described in Section [-@sec-importance-distribution].

### Spatial distribution {#sec-importance-distribution}

In order to estimate spatial distribution of variable importance values, I applied two different approaches.
First of them is based on the raster aggregation - resampling of satellite imagery from 30 m to 1.5 km resolution.
Lowering the resolution of the data and averaging band values highly decreases computational time, as well as helps to discover more general trends and patterns rather than local ones.
After resampling, Shapley values are calculated for every raster cell and variable importance has measured.

The second approach utilizes LUCAS training points used during a model training together with spatial interpolation techniques.
First, Shapley values are calculated for every point and importance of variable is assigned to them.
This step is followed by spatial interpolation of variable importance values from points to continuous raster layer with the help of the Inverse Distance Weighting (IDW) interpolation method.

Both approaches have their pros and cons.
Raster aggregation method is spatially more consistent, but averaging of spectral values may not entirely represent objects on the ground.
On the other hand, point interpolation method is very accurate for places near LUCAS points location, but values for more distant areas may not be as reliable.

## R language environment {#sec-r}

Almost every step of analysis described in previous sections was performed with the use of R [@R-base] - an open-source programming language designed mainly for statistical computing and visualizing data.
I used RStudio [@rstudio_team_rstudio_2020] as an integrated development environment (IDE).
Apart from base R functionalities, a number of packages created by the R community were implemented into workflow.
I used *terra* package [@R-terra] to perform raster data operations and *sf* [@R-sf] to manipulate and process vector data.
To conduct machine learning steps of the analysis, I used an environment of various machine learning packages called *mlr3* [@R-mlr3].
Random forest algorithm used by *mlr3* framework is part of the *ranger* package [@R-ranger].
I also used *dplyr* [@R-dplyr] and *tidyr* packages [@R-tidyr] to clean and process tabular data.
*DALEX* [@R-DALEX] and *DALEXtra* [@R-DALEXtra] packages provided various functionalities enabling me to estimate variable importance and visualize these results with the help of the *ggplot2* package [@R-ggplot2].
Moreover, the *corrplot* package was used to calculate and visualize correlation matrix of Landsat data.
Package called *gstat* [@R-gstat] helped to interpolate variable importance values from points to a continuous raster layer.
In addition, the *future* package [@R-future] was used to enable multi-threading of some computationally intensive tasks.

```{r}
#| label: pakietbib
#| echo: false
#| warning: false
pakiety = c("base", "kableExtra", "terra", "sf", "mlr3", "ranger", "ggplot2", "dplyr", "tidyr", "DALEX", "DALEXtra", "future", "gstat")
knitr::write_bib(pakiety, "packages.bib", width = 60)
```
