# Materials and methods {#sec-data-methods}

Workflow of the study consisted of several stages: preprocessing of source data (described in Sections [-@sec-sat] and [-@sec-landcover]), creating training dataset, model's parameters tuning, land cover map prediction, model quality assessment and evaluating the impact of thermal band on the model's results (Figure [-@fig-rycina4]).
Each of these steps was performed using R programming language [@R-base].
Final visualisations were created in QGIS software [@qgis_development_team_qgis_2009].
Both programming environment and GIS software used in this process are open-source.

```{mermaid}
%%| label: fig-rycina4
%%| echo: false
%%| fig-cap: "Mermaid flowchart"
flowchart LR
  A[Hard edge] --> B(Round edge)
  B --> C{Decision}
  C --> D[Result one]
  C --> E[Result two]
  F[Hard edge] --> G(Round edge)
  F --> B
  G --> H{Decision}
  H --> I[Result one]
  H --> J[Result two]
```

Landsat ARD dataset, provided by GLAD laboratory at the Univeristy of Maryland, was used as a source of multi-spectral satellite imagery.
Training points were obtained from LUCAS dataset created by Eurostat [@dandrimont_harmonised_2020].
Both data sets were downloaded for central-western part of Poland which was chosen as training area (Figure [-@fig-rycina1]).
This data was pre-processed and then used to train the model and validate its performance.

```{r}
#| label: fig-rycina1
#| echo: false
#| fig-cap: "Training area"
#| out-width: "100%"
#| out-height: 400px
knitr::include_graphics("figures/study_area.png")
```

## Satellite imagery {#sec-sat}

Satellite imagery from GLAD Landsat ARD is available in 16-day interval composites and is divided into 1° x 1° tiles.
Processing of original Landsat images performed by GLAD team included converting spectral bands to top-of-atmosphere (TOA) reflectance, converting thermal band to brightness temperature (BT) in Kelvins, scaling the values of all bands as well as adding quality flag for every pixel [@potapov_landsat_2020].

Satellite images for eight 1° x 1° tiles, covering the study area (Figure [-@fig-rycina1]), were downloaded using GLAD Tools v1.1 and PERL programming language.
These images are from 10th interval of the year 2018, so downloaded mosaics consist of images created between 24.05.2018 and 8.06.2018.
All downloaded images were merged and reprojected from WGS84 coordinate reference system (EPSG:4326) to UTM zone 33N (EPSG:32633).
Every band was also resampled from 0.00025° resolution (corresponding to 27.83 m on equator) to 30 meters resolution.

In addition, four spectral indices were derived: Normalized Difference Vegetation Index (NDVI), Modified Normalized Difference Water Index (MNDWI), Normalized Difference Moisture Index (NDMI) and Modified Bare soil Index (MBI).
Formulas used to calculate these indices can be found in Table [-@tbl-tabela1].

```{r tabela1, echo=FALSE}
#| label: tbl-tabela1
#| echo: false
#| tbl-cap: "Formulas of spectral indices dervied from Landsat data"
df = data.frame(
  a = c("Blue", "Green", "Red", "Near Infrared", "Short-wave Infrared 1", "Short-wave Infrared 2", "Thermal", "Normalized Difference Vegetation Index", "Modified Normalized Difference Water Index", "Normalized Difference Moisture Index", "Modified Bare Surface Index"),
  b = c("B2", "B3", "B4", "B5 (NIR)", "B6 (SWIR1)", "B7 (SWIR2)", "B10 (TIRS1)", "NDVI", "MNDWI", "NDMI", "MBI"),
  c = c(rep("-", 7), "(B5 -B4) / (B4 + B5)", "(B3 - B6) / (B3 + B6)", "(B5 - B6) / (B5 + B6)", "(B6 - B7 - B5) / (B6 + B7 + B5) + 0.5")
)
colnames(df) = c("band/index", "abbreviation", "formula")
df |>
  knitr::kable(format = "latex", booktabs = TRUE, linesep = "") |>
  kableExtra::row_spec(0, bold = TRUE, extra_css = "border-left: 1px solid grey; border-right: 1px solid grey") |>
  kableExtra::kable_classic(full_width = FALSE) |>
  kableExtra::row_spec(1:(nrow(df) - 1), extra_latex_after = "\\hline") |>
  kableExtra::column_spec(1, border_left = TRUE, width = "4cm") |>
  kableExtra::column_spec(2, border_left = TRUE) |>
  kableExtra::column_spec(3, border_left = TRUE, border_right = TRUE)
```

## Land cover data {#sec-landcover}

Data collected during LUCAS survey performed by Eurostat was chosen as land cover training set.
At the moment of writing, it is the most accurate and comprehensive dataset containing information about land use and land cover [@pflugmacher_mapping_2019] due to the fact, that every point was either manually photo-interpreted or assessed during *in-situ* visit.

LUCAS survey consists of two phases.
First phase is based on grid of points with 2km spacing covering whole territory of the European Union (which equals to more than 1 million points).
Each point of the grid is visually interpreted using ortho-photos or satellite images, and classified into one of seven major land-cover classes.
These classes are: arable land, permanent crops, grassland, wooded areas/shrub land, bare land, artificial land and water.
In the second phase, a subsample of grid points is selected and then visited by Eurostat surveyors.
They classify each point according to full LUCAS land cover and land use classification.
The survey takes place in the spring and summer in order to observe chosen places in high vegetation season [@dandrimont_harmonised_2020].

Surveyor not only assign land cover and land use classes to points, but they also add auxillary information such as plant species present at the site, percentage of land coverage for a chosen class, height of the trees and their maturity, as well as information about water management and irrigation.
If there are more than one land cover/land use types at the point, observer can also assign a secondary class for every LUCAS point.

Majority of the training points used for the classification model were points from the second phase of LUCAS survey, also called LUCAS Primary Data.
I downloaded a total of 4,153 points for the study area.
Pre-processing step included omitting records with missing data, excluding artificial linear land cover classes (e.g. roads or railways) and excluding points that were surveyed more than 500 meters from their theoretical location.
In the next step, detailed land cover classes were aggregated into eight main groups of land cover types.
Two of them - grassland and shrubland were additionally aggregated into one land cover class.
Then, I filtered some of the classes according to the percentage of land coverage or percentage of impervious surface coverage (Table [-@tbl-tabela2]).

```{r tabela2, echo=FALSE}
#| label: tbl-tabela2
#| echo: false
#| tbl-cap: "Filters applied to reclassified land cover groups. IMP - impervious surface, HRB - herbaceous plants cover, TC - tree cover"
df = data.frame(
  ID = c("1", "2", "3", "4", "5", "6", "7"),
  land_cover = c("arable land", "grasslands", "forests", "bare land", "artificial land", "water bodies", "wetlands"),
  lucas_grid = c(rep("-", 3), "6 (Bare surface)", "7 (Artificial areas)", "8 (Inland water)", "-"),
  lucas_primary = kableExtra::linebreak(c("B00 (Cropland)", "E00 (Grassland), D00 (Shrubland)", "C00 (Woodland)", "F00 (Bare land)", "A00 (Artificial land)", "G00 (Water areas)", "H00 (Wetlands)"), double_escape = F, linebreaker = "\n"),
  filters = c("<30% IMP", ">50% HRB; <30% IMP", ">50% TC; <20% IMP", "-", ">70% IMP", "-", "-")
)
colnames(df) = c("ID", "LC class", "LUCAS Grid", "LUCAS Primary Data", "Filters")
cols = c("#e8ef5f", "#80dc59", "#11a723", "#b7b7b7", "#ea001f","#56a4f3", "#7a338c")
df |>
  knitr::kable(format = "latex", booktabs = TRUE, linesep = "") |> 
  kableExtra::row_spec(0, bold = TRUE, extra_css = "border-left: 1px solid grey; border-right: 1px solid grey") |>
  kableExtra::column_spec(1:3, border_left = TRUE) |>
  kableExtra::column_spec(4, width = "4cm", border_left = TRUE) |>
  kableExtra::column_spec(5, width = "2cm", border_left = TRUE, border_right = TRUE) |>
  kableExtra::column_spec(1, bold = TRUE, background = cols) |>
  kableExtra::kable_classic(full_width = FALSE) |>
  kableExtra::row_spec(1:(nrow(df) - 1), extra_latex_after = "\\hline")
```

For the least frequent classes in the LUCAS Primary Data dataset - bare land, artificial land and water bodies - I also added points classified during the first phase of LUCAS survey (Figure [-@fig-rycina2]).
This step was necessary to ensure that every land cover class is represented by enough number of points.
It was not possible only for wetlands class, because of lack of such category in the first phase classification.
At the end of the pre-processing, dataset was left with 3,778 training points [@oliver_buck_analysis_2015].

```{r}
#| label: fig-rycina2
#| echo: false
#| fig-cap: "Distribution of points by land cover class after pre-processing"
#| out-width: "100%"
#| out-height: 350px
knitr::include_graphics("figures/lucas_data.png")
```

After extracting values from Landsat ARD raster, LUCAS points were also filtered using quality flag provided.
Only points with clear-sky quality flag were taken into account during the process of model training.
Moreover, water bodies points in which NDWI was lower than 0 were also excluded.
These two conditions eliminated over 400 points in total.

Training set obtained after pre-processing can be seen in Figure [-@fig-rycina3].
Spatial distribution of data points was fairly even and due to the structure of LUCAS data set, every point was located at least 2 kilometers from another one.

```{r}
#| label: fig-rycina3
#| echo: false
#| fig-cap: "Spatial distribution of LUCAS training points after pre-processing"
#| out-width: "100%"
#| out-height: 400px
knitr::include_graphics("figures/lucas_distribution.png")
```

## Machine learning {#sec-ml}

Machine learning is a computation method used to teach machines to learn from datasets automatically without being specifically programmed (@mahesh_machine_2018; @sarker_machine_2021).
We can divide machine learning methods into two main groups: supervised and unsupervised.

Unsupervised learning analyzes unlabeled datasets without the need for human intervention.
This is widely used for extracting generative features, identifying meaningful trends and structures, grouping results and exploratory purposes [@sarker_machine_2021].
This type of machine learning discovers hidden patterns or data groupings (clusters) which is used in exploratory analysis or objects segmentation.
The most common unsupervised learning algorithm is clustering.
REF NEEDED

Supervised learning uses labeled training data and a collection of training examples, which are used by an algorithm to find relationships between different variables used to describe training data.
It is carried out when certain goals are identified to be accomplished from a certain set of inputs.
There are two types of supervised learning tasks: classification (seperating data) and regression (fitting data) [@sarker_machine_2021].

In this study, supervised classification algorithm called Random Forest (RF) was used [@breiman_random_2001].

### Random forest algorithm {#sec-rf}

I chose Random Forest as an algorithm used in this study.
It is a very popular machine learning tool thanks to its high interpretability and relatively high accuracy [@qi_random_2012].
Other advantages of this algorithm is its ability to handle missing values, wide spectrum of accepted variable types (continuous, binary, categorical) and ease of modelling high-dimensional data [@qi_random_2012].
Random Forest consists of a specified number of decision trees, which are based on series of splitting rules.

Decision tree aims to partition the dataset into smaller, more homogeneous groups [@kuhn_applied_2013].
This process creates set of rules by dividing dataset into several categories.
Each rule in the decision tree is specified by a feature (variable used to split) and a threshold (value of a feature dividing dataset) [@sekulic_random_2020].
Random forest algorithm is characterized by using many decision trees at the same time and receiving results by applying majority voting system based on outputs of all decision trees [@kuhn_applied_2013].
Each tree in the forest has slightly different input data - a subset of data is sampled with replacement to get different result in every tree.
This process is known as bagging or bootstrap aggregating [@schonlau_random_2020].

### Parameter tuning {#sec-tuning}

-   what is tuning of model's parameters

### Model quality assessment {#sec-resampling}

-   measures and indices of classification model quality

-   idea of resampling

-   idea of nested resampling

-   spatial cross-validation and its purpose

## R language environment {#sec-r}

Almost every step of analysis described in previous sections was performed with use of R [@R-base] - an open-source programming language designed mainly for statistical computing and visualizing data.
I used RStudio [@rstudio_team_rstudio_2020] as an integrated development environment (IDE) created for R.
Apart from base R functionalities, I used a number of packages created by R community.
We used *terra* package [@R-terra] to perform raster data operations and *sf* [@R-sf] to manipulate and process vector data.
To conduct machine learning steps of the analysis, we used whole environment of various machine learning packages called *mlr3* [@R-mlr3].
Random forest algorithm used by *mlr3* framework is part of *ranger* package [@R-ranger] .
I also used *dplyr* [@R-dplyr] and *tidyr* packages [@R-tidyr] to clean and process tabular data.
*DALEX* [@R-DALEX] and *DALEXtra* [@R-DALEXtra] packages provided various functionalities enabling me to estimate variable importance and visualize these results with the help of *ggplot2* package [@R-ggplot2].
Package called *gstat* [@R-gstat] helped to interpolate variable importance values from points to continuous raster layer.
In addition, *future* package [@R-future] was used to enable multi-threading of some computationally intensive tasks.

```{r}
#| label: pakietbib
#| echo: false
#| warning: false
pakiety = c("base", "kableExtra", "terra", "sf", "mlr3", "ranger", "ggplot2", "dplyr", "tidyr", "DALEX", "DALEXtra", "future", "gstat")
knitr::write_bib(pakiety, "packages.bib", width = 60)
```
